{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import pytesseract \n",
    "import sys \n",
    "from pdf2image import convert_from_path \n",
    "import os \n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "PDF_file = \"file_1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = convert_from_path(\"file_3.pdf\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aastha\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aastha\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aastha Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Aastha\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Aastha\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dadasaheb Pandharinath Garad', 'Tata Magic No', 'Plea', 'Hotel Abhishek', 'Suresh Punjaram Sable', 'Name', 'B No']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "stop = stopwords.words('english')\n",
    "def ie_preprocess(document):\n",
    "    document = ' '.join([i for i in document.split() if i not in stop])\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "def extract_names(document):\n",
    "    names = []\n",
    "    sentences = ie_preprocess(document)\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                if chunk.label() == 'PERSON':\n",
    "                    names.append(' '.join([c[0] for c in chunk]))\n",
    "    return names\n",
    "image_counter = 1\n",
    "# Iterate through all the pages stored above \n",
    "for page in pages: \n",
    "  \n",
    "    # Declaring filename for each page of PDF as JPG \n",
    "    # For each page, filename will be: \n",
    "    # PDF page 1 -> page_1.jpg \n",
    "    # PDF page 2 -> page_2.jpg \n",
    "    # PDF page 3 -> page_3.jpg \n",
    "    # .... \n",
    "    # PDF page n -> page_n.jpg \n",
    "    filename = \"page_\"+str(image_counter)+\".jpg\"\n",
    "      \n",
    "    # Save the image of the page in system \n",
    "    page.save(filename, 'JPEG') \n",
    "  \n",
    "    # Increment the counter to update filename \n",
    "    image_counter = image_counter + 1\n",
    "  \n",
    "''' \n",
    "Part #2 - Recognizing text from the images using OCR \n",
    "'''\n",
    "\n",
    "# Variable to get count of total number of pages \n",
    "filelimit = image_counter-1\n",
    "  \n",
    "# Creating a text file to write the output \n",
    "outfile = \"out_text.txt\"\n",
    "  \n",
    "# Open the file in append mode so that  \n",
    "# All contents of all images are added to the same file \n",
    "f = open(outfile, \"a\") \n",
    "  \n",
    "# Iterate from 1 to total number of pages \n",
    "for i in range(1, filelimit + 1): \n",
    "  \n",
    "    # Set filename to recognize text from \n",
    "    # Again, these files will be: \n",
    "    # page_1.jpg \n",
    "    # page_2.jpg \n",
    "    # .... \n",
    "    # page_n.jpg \n",
    "    filename = \"page_\"+str(i)+\".jpg\"\n",
    "          \n",
    "    # Recognize the text as string in image using pytesserct \n",
    "    text = str(((pytesseract.image_to_string(Image.open(filename))))) \n",
    "    f.write(text) \n",
    "f.close() \n",
    "f = open(outfile, \"r\") \n",
    "text = f.read()\n",
    "f.close()\n",
    "names = list(set(extract_names(text)))\n",
    "print(names)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suresh Punjaram Sable\n",
      "Convited\n",
      "07/07/2014\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    if len(re.findall(r'accused\\s*'+name,text)) != 0:\n",
    "        print(name)\n",
    "text = text.lower()\n",
    "if re.search(\"acquited\",text):\n",
    "    print(\"Acquited\")\n",
    "elif re.search(\"convicted\",text):\n",
    "    print(\"Convited\")   \n",
    "print(re.findall(r'(\\d+/\\d+/\\d+)',text)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['177', '192', '66', '158']\n"
     ]
    }
   ],
   "source": [
    "l = re.findall(r'sec.\\w*\\s*\\d*',text)\n",
    "lt = ' '.join(map(str, l)) \n",
    "s = list(set(re.sub(r'sec.\\w*','',lt).split()))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
