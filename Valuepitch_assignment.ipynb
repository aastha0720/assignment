{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment I will be using the Pytesseract library to convert the given PDF into images, and then use OCR to recognize the content of the image and store it in a text file.\n",
    "* First the PDF will be converted into image files. Each page of the PDF will be stored as a seperate image.\n",
    "* Then using Pytesseract, the content of the image will be recognized and stored in a text file.\n",
    "\n",
    "Then using NLTK and regex, the required values will be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import pytesseract \n",
    "import sys \n",
    "from pdf2image import convert_from_path \n",
    "import os \n",
    "import time\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "pages = convert_from_path(\"file_3.pdf\", 500)\n",
    "image_counter = 1\n",
    "# Iterate through all the pages stored above and save them a JPEG files\n",
    "for page in pages: \n",
    "  \n",
    "    filename = \"page_\"+str(image_counter)+\".jpg\"\n",
    "    page.save(filename, 'JPEG') \n",
    "    image_counter = image_counter + 1\n",
    "  \n",
    " \n",
    "# Recognizing text from the images using OCR and storing in a timestamped text file\n",
    "filelimit = image_counter-1\n",
    "outfile = \"out_text_\"+timestr+\".txt\"\n",
    "f = open(outfile, \"a\") \n",
    "for i in range(1, filelimit + 1): \n",
    "    filename = \"page_\"+str(i)+\".jpg\"      \n",
    "    # Recognize the text as string in image using pytesserct \n",
    "    text = str(((pytesseract.image_to_string(Image.open(filename))))) \n",
    "    f.write(text) \n",
    "f.close() \n",
    "\n",
    "# Read the text file created and store the content in a variable\n",
    "f = open(outfile, \"r\") \n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aastha\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aastha\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aastha Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Aastha\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Aastha\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "stop = stopwords.words('english')\n",
    "def ie_preprocess(document):\n",
    "    document = ' '.join([i for i in document.split() if i not in stop])\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "def extract_names(document):\n",
    "    names = []\n",
    "    sentences = ie_preprocess(document)\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                if chunk.label() == 'PERSON':\n",
    "                    names.append(' '.join([c[0] for c in chunk]))\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the accused is:  Suresh Punjaram Sable\n",
      "Address of the accused:  Valadgaon, Tal. Aurangabad\n",
      "Sections under which the accused was charged:  ['192', '158', '66', '177']\n",
      "Whether is accused is convicted or acquited:  Convited\n",
      "Date of verdict:  07/07/2014\n"
     ]
    }
   ],
   "source": [
    "# Use the extract names method from nltk Library to extract all the possible names in the text file\n",
    "names = list(set(extract_names(text)))\n",
    "  \n",
    "accused = ''\n",
    "#From the list of names generated from extract_names method find the name of the accused\n",
    "for name in names:\n",
    "    if len(re.findall(r'accused\\s*'+name,text)) != 0:\n",
    "        accused = name\n",
    "\n",
    "# Finding the address of the accused\n",
    "a= re.findall(r'r/o:?(.*)\\n\\n(?:5.|par)',text,re.S | re.I)\n",
    "address = a[0].replace('\\n',', ')\n",
    "\n",
    "# Finding whether the accused is convicted or acquited\n",
    "final_decision = ''\n",
    "if re.search(\"acquited\",text, re.I):\n",
    "    final_decision = \"Acquited\"\n",
    "elif re.search(\"convicted\",text, re.I):\n",
    "    final_decision = \"Convited\"  \n",
    "\n",
    "# Date of Decisopn    \n",
    "date = re.findall(r'(\\d+/\\d+/\\d+)',text)[-1]\n",
    "\n",
    "# Sections under which the accused was charged\n",
    "l = re.findall(r'sec.-?\\w*-?\\s*(\\d*)',text, re.I)\n",
    "lt = ' '.join(map(str, l)) \n",
    "sections = list(set(re.sub(r'sec.\\w*','',lt).split()))\n",
    "                    \n",
    "                    \n",
    "print(\"Name of the accused is: \",accused)\n",
    "print(\"Address of the accused:\",address)\n",
    "print(\"Sections under which the accused was charged: \",sections)\n",
    "print(\"Whether is accused is convicted or acquited: \",final_decision)\n",
    "print(\"Date of verdict: \",date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
